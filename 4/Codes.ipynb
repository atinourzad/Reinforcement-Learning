{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "class PolicyGradRBF(object):\n",
    "    \n",
    "    def __init__(self, __env, __actions, __stateCenters, __actionCenters, __stateSigma, __actionSigma):\n",
    "        self.env = __env\n",
    "        self.actions = __actions\n",
    "        self.stateCenters = __stateCenters\n",
    "        self.actionCenters = __actionCenters\n",
    "        self.stateSigma = __stateSigma\n",
    "        self.actionSigma = __actionSigma\n",
    "        \n",
    "        self.numActions = len(__actions)\n",
    "        self.numStateCenters = len(__stateCenters)\n",
    "        self.numActionCenters = len(__actionCenters)\n",
    "        \n",
    "        \n",
    "        self.teta = np.zeros((self.numActionCenters,1))\n",
    "        self.w = np.zeros((self.numStateCenters,1))\n",
    "        self.actionPhi = np.zeros((self.numActionCenters))\n",
    "        self.statePhi = np.zeros((self.numStateCenters))\n",
    "        \n",
    "        \n",
    "    def createRBFState(self, state, sigma, centers):\n",
    "        center = np.array([centers[0], centers[1]])\n",
    "        \n",
    "        state = np.array([(state[0]+1.2)/1.7, (state[1]+0.07)/0.14])\n",
    "        \n",
    "        dist = (state - center).reshape(2,1)\n",
    "        distT = dist.reshape(1,2)\n",
    "        b = distT.dot(sigma)\n",
    "        b = b.dot(dist)\n",
    "        \n",
    "        return np.exp(-b*b)\n",
    "    \n",
    "    def createRBFAction(self, state, sigma, centers, action):\n",
    "        center = np.array([centers[0], centers[1], centers[2]])\n",
    "        \n",
    "        state = np.array([(state[0]+1.2)/1.7, (state[1]+0.07)/0.14, action])\n",
    "        \n",
    "        dist = (state - center).reshape(3,1)\n",
    "        distT = dist.reshape(1,3)\n",
    "        b = distT.dot(sigma)\n",
    "        b = b.dot(dist)\n",
    "        \n",
    "        return np.exp(-b*b)\n",
    "\n",
    "    def createActionPhi(self, state, action):\n",
    "        self.actionPhi = np.zeros((self.numActionCenters))\n",
    "        for i in range(self.numActionCenters):\n",
    "            self.actionPhi[i] = self.createRBFAction(state, self.actionSigma, self.actionCenters[i], action)\n",
    "            \n",
    "    def createStatePhi(self, state):\n",
    "        self.statePhi = np.zeros((self.numStateCenters))\n",
    "        for i in range(self.numStateCenters):\n",
    "            self.statePhi[i] = self.createRBFState(state, self.stateSigma, self.stateCenters[i])\n",
    "            \n",
    "    def createStateActionValue(self, state, action):\n",
    "        self.createActionPhi(state, action)\n",
    "        return self.teta.T.dot(self.actionPhi)\n",
    "    \n",
    "    def createStateActionsValues(self, state):\n",
    "        values = []\n",
    "        \n",
    "        for action in self.actions:\n",
    "            values.append(self.createStateActionValue(state, action))\n",
    "            \n",
    "        return values\n",
    "    \n",
    "    def calcPref(self, state):\n",
    "        stateActionsValues = self.createStateActionsValues(state)\n",
    "        pref = []\n",
    "        \n",
    "        for value in stateActionsValues:\n",
    "            pref.append(np.exp(value))\n",
    "            \n",
    "        return pref\n",
    "    \n",
    "    def calcProbActions(self, state):\n",
    "        pref = self.calcPref(state)\n",
    "        normFactor = sum(pref)\n",
    "        return [float(num)/normFactor for num in pref]\n",
    "    \n",
    "    def chooseBestAction(self, state):\n",
    "        stateActionsValues = self.createStateActionsValues(state)\n",
    "        prob = self.calcProbActions(state)\n",
    "        index = np.random.choice(np.arange(len(stateActionsValues)), 1, prob)\n",
    "        \n",
    "        return self.actions[index[0]]\n",
    "    \n",
    "    def calcStateValue(self, state):\n",
    "        self.createStatePhi(state)\n",
    "        \n",
    "        statePhiT = self.statePhi.reshape(self.numStateCenters,1)\n",
    "        \n",
    "        return self.w.T.dot(statePhiT)\n",
    "    \n",
    "    def calcPolicy(self, state, action):\n",
    "        stateActionValue = self.createStateActionValue(state, action)\n",
    "        pref = self.calcPref(state)\n",
    "        normFactor = sum(pref)\n",
    "        \n",
    "        return np.exp(stateActionValue)/normFactor\n",
    "    \n",
    "    def calcGradPolicy(self, state, action):\n",
    "        const = 0\n",
    "        for iterAction in self.actions:\n",
    "            self.createActionPhi(state, iterAction)\n",
    "            const += self.calcPolicy(state, action) * self.actionPhi\n",
    "            \n",
    "        self.createActionPhi(state, action)\n",
    "        return (self.actionPhi-const).reshape(self.numActionCenters,1)\n",
    "    \n",
    "    def updateW(self, state, alphaW, delta, gammaNew):\n",
    "        self.createStatePhi(state)\n",
    "        statePhiT = self.statePhi.reshape(self.numStateCenters,1)\n",
    "        self.w += alphaW * delta * gammaNew * statePhiT\n",
    "    \n",
    "    def updateTeta(self, alphaTeta, delta, gammaNew, gradPolicy):\n",
    "        self.teta += alphaTeta * gammaNew * delta * gradPolicy\n",
    "\n",
    "            \n",
    "    def calcDelta(self, nextState, currState, reward, gamma):\n",
    "        currStateValue = self.calcStateValue(currState)\n",
    "        nextStateValue = self.calcStateValue(nextState)\n",
    "        \n",
    "        return reward + gamma * nextStateValue - currStateValue\n",
    "    \n",
    "    def doAlgorithm(self, gamma, alphaTeta, alphaW):\n",
    "        \n",
    "        currState = self.env.reset()\n",
    "        currAction = self.chooseBestAction(currState)\n",
    "        gammaNew = 1  \n",
    "        while currState[0] < 0.5:\n",
    "            self.env.render()\n",
    "            \n",
    "            nextState, reward, done, info = self.env.step(currAction)\n",
    "            \n",
    "            if nextState[0] > 0.49:\n",
    "                reward = 200\n",
    "\n",
    "            nextAction = self.chooseBestAction(nextState)\n",
    "            \n",
    "            delta = self.calcDelta(nextState, currState, reward, gamma)\n",
    "            \n",
    "            self.updateW(currState, alphaW, delta, gammaNew)\n",
    "            gradPolicy = self.calcGradPolicy(currState, currAction)\n",
    "            self.updateTeta(alphaTeta, delta, gammaNew, gradPolicy)\n",
    "            \n",
    "            currState = nextState\n",
    "            currAction = nextAction\n",
    "            gammaNew = gammaNew * gamma\n",
    "            \n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "positionCenter = np.linspace(-1.2, 0.6, 5)\n",
    "velocityCenter = np.linspace(-0.07, 0.07, 3)\n",
    "actionCentersVal = [0.5, 1.5]\n",
    "\n",
    "stateCenters = []\n",
    "actionCenters = []\n",
    "for posCent in positionCenter:\n",
    "    for velCent in velocityCenter:\n",
    "        stateCenters.append((posCent,velCent))\n",
    "        for actCenter in actionCentersVal:\n",
    "            actionCenters.append((posCent,velCent, actCenter))\n",
    "\n",
    "actions = [0,1,2]  \n",
    "\n",
    "stateSigma = np.zeros((2,2))\n",
    "stateSigma[0,0] = 1\n",
    "stateSigma[1,1] = 1\n",
    "\n",
    "actionSigma = np.zeros((3,3))\n",
    "actionSigma[0,0] = 1\n",
    "actionSigma[1,1] = 1\n",
    "actionSigma[2,2] = 1\n",
    "\n",
    "gamma = 0.9\n",
    "alphaW = 0.1/50\n",
    "alphaTeta = 0.1/50\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "myalgo = PolicyGradRBF(env, actions, stateCenters, actionCenters, stateSigma, actionSigma)\n",
    "myalgo.doAlgorithm(gamma, alphaTeta, alphaW)\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print(' \\n Reached the top in {:.2f}s '.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
