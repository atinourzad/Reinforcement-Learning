{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run FrozenLake.ipynb\n",
    "\n",
    "env = FrozenLakeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-step SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nStepSARSA(object):\n",
    "    def __init__(self, __env, __n, __maxNumEp):\n",
    "        self.env = __env\n",
    "        self.n = __n\n",
    "        self.maxNumEp = __maxNumEp\n",
    "        self.numEp = 0\n",
    "        self.qValues = np.zeros((env.observation_space.n,self.env.action_space.n))\n",
    "        self.storedStates = {}\n",
    "        self.storedActions = {}\n",
    "        self.storedRewards = {}\n",
    "        self.endOfEp = False\n",
    "        self.regret = []\n",
    "        self.sumRewards = 0\n",
    "        self.maxReward = 0\n",
    "\n",
    "\n",
    "    def behavPolicy(self, currState, ep):\n",
    "        numAction = self.env.action_space.n\n",
    "\n",
    "        pValues = [ep/numAction] * (numAction)\n",
    "\n",
    "        ag = np.argmax(self.qValues[currState])\n",
    "        pValues[ag] = pValues[ag] + 1 - ep\n",
    "\n",
    "        return np.random.choice(np.arange(len(pValues)), p=pValues)\n",
    "\n",
    "    def resetStateActionRewards(self):\n",
    "        self.storedStates = {}\n",
    "        self.storedActions = {}\n",
    "        self.storedRewards = {}\n",
    "    \n",
    "    def storeState(self, currState, t):\n",
    "        self.storedStates[(t+1)%(self.n)] = currState\n",
    "\n",
    "    def storeAction(self, currAction, t):\n",
    "        self.storedActions[(t+1)%(self.n)] = currAction\n",
    "\n",
    "    def storeReward(self, currReward, t):\n",
    "        self.storedRewards[(t+1)%(self.n)] = currReward\n",
    "\n",
    "    def takeCurrAction(self, currAction):\n",
    "        nextState, nextReward, done, info = self.env.step(currAction)\n",
    "        self.endOfEp = done\n",
    "\n",
    "        return [nextState, nextReward]\n",
    "\n",
    "    def updateQValues(self, tau, alpha, discReturn):\n",
    "        sTau = self.storedStates[tau%(self.n)]\n",
    "        aTau = self.storedActions[tau%(self.n)]\n",
    "\n",
    "        self.qValues[sTau][aTau] += alpha * (discReturn - self.qValues[sTau][aTau])\n",
    "        \n",
    "    def findEpRegret(self, numEp):\n",
    "        epRegret = self.maxReward * numEp - (self.sumRewards)\n",
    "        self.regret.append(epRegret)\n",
    "        \n",
    "    def plotRegrets(self):\n",
    "        plot = plt.plot([i for i in range(1,len(self.regret)+1)], self.regret)\n",
    "        plt.setp(plot, 'color', 'orchid')\n",
    "        plt.show()\n",
    "\n",
    "    def doAlgorithm(self):\n",
    "        \n",
    "        for ep in range(self.maxNumEp):\n",
    "            self.resetStateActionRewards()\n",
    "            \n",
    "            currState = self.env.reset()\n",
    "            self.storeState(currState, -1)\n",
    "\n",
    "            currAction = self.behavPolicy(currState, 1)\n",
    "            self.storeAction(currAction, -1)\n",
    "\n",
    "            tau = 0\n",
    "            t = -1\n",
    "            gamma = 1\n",
    "            T = sys.maxsize\n",
    "            \n",
    "            epReward = 0 \n",
    "\n",
    "            while tau < (T-1):\n",
    "                t += 1\n",
    "        \n",
    "                if t < T:\n",
    "                    \n",
    "                    #Take action and observe next state and reward\n",
    "                    nextState, nextReward = self.takeCurrAction(self.storedActions[t%(self.n)])\n",
    "                    self.storeState(nextState, t)\n",
    "                    self.storeReward(nextReward, t)\n",
    "                    self.sumRewards += nextReward\n",
    "                    epReward += nextReward\n",
    "\n",
    "                    if self.endOfEp:\n",
    "                        T = t+1\n",
    "\n",
    "                    else:\n",
    "                        nextAction = self.behavPolicy(nextState, 1/(ep+1))\n",
    "                        self.storeAction(nextAction, t)\n",
    "\n",
    "                tau = t - self.n + 1\n",
    "\n",
    "                if tau >= 0:\n",
    "                    discReturn = np.sum([gamma**(i-tau-1)*(self.storedRewards[i%(self.n)]) for i in range(tau+1, min(tau+(self.n),T))])\n",
    "\n",
    "                    if tau+(self.n) < T:\n",
    "                        discReturn += (gamma**(self.n))*(self.qValues[self.storedStates[(tau)%(self.n)]][self.storedActions[(tau)%(self.n)]])\n",
    "\n",
    "                    self.updateQValues(tau, 0.1, discReturn)\n",
    "                    \n",
    "            if self.maxReward < epReward:\n",
    "                self.maxReward = epReward\n",
    "                \n",
    "            self.findEpRegret(ep)\n",
    "\n",
    "        return self.regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEp = 400\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "n1 = 1\n",
    "myNStepSATSA = nStepSARSA(env, n1, numEp)           \n",
    "regret1 = myNStepSATSA.doAlgorithm()\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print(' \\n {} pisodes completed in {:.2f}s for n=1'.format(numEp, elapsed_time))\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "n2 = 5\n",
    "myNStepSATSA = nStepSARSA(env, n2, numEp)           \n",
    "regret2 = myNStepSATSA.doAlgorithm()\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print(' \\n {} pisodes completed in {:.2f}s for n=5'.format(numEp, elapsed_time))\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "n3 = 100\n",
    "myNStepSATSA = nStepSARSA(env, n3, numEp)           \n",
    "regret3 = myNStepSATSA.doAlgorithm()\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print(' \\n {} pisodes completed in {:.2f}s for n=100'.format(numEp, elapsed_time))\n",
    "\n",
    "# plot1, = plt.plot([i for i in range(len(regret1))], regret1)\n",
    "# plt.setp(plot1, 'color', 'orchid')\n",
    "\n",
    "# plot2, = plt.plot([i for i in range(len(regret2))], regret2)\n",
    "# plt.setp(plot2, 'color', 'darkorchid')\n",
    "\n",
    "# plot3, = plt.plot([i for i in range(len(regret3))], regret3)\n",
    "# plt.setp(plot3, 'color', 'blue')\n",
    "\n",
    "# plt.legend([plot2,plot3],[\"n = 5\", 'n=100'])\n",
    "# plt.title('Regret for different values of n')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA(lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def epGreedy(stateQValues, numAction, ep):\n",
    "    pValues = [ep/numAction] * (numAction)\n",
    "    \n",
    "    ag = np.argmax(stateQValues)\n",
    "    \n",
    "    pValues[ag] = pValues[ag] + 1 - ep\n",
    "   \n",
    "    return pValues\n",
    "\n",
    "def chooseBestAction(pValues):\n",
    "    return np.random.choice(np.arange(len(pValues)), p=pValues)\n",
    "\n",
    "def plotForMe(myInput):\n",
    "    plot = plt.plot([(i) for i in range(1,len(myInput)+1)], myInput)\n",
    "    plt.setp(plot, 'color', 'orchid')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class SARSALambda(object):\n",
    "    \n",
    "    def __init__(self, __env, __lambda, __numEp):\n",
    "        self.env = __env\n",
    "        self.algoLambda = __lambda\n",
    "        self.gamma = 1\n",
    "        self.numEpisode = __numEp\n",
    "        self.endOfEp = False\n",
    "        \n",
    "        self.currAction = -1\n",
    "        self.currState = -1\n",
    "        self.currEpSteps = 0\n",
    "        \n",
    "        self.totalRewards = 0\n",
    "        self.maxReward = 0\n",
    "        \n",
    "        self.qValues = np.zeros((env.observation_space.n,self.env.action_space.n))\n",
    "        self.eligibility = np.zeros((env.observation_space.n,self.env.action_space.n))\n",
    "        self.allRegrets = []\n",
    "        \n",
    "    \n",
    "    def takeCurrAction(self):\n",
    "        nextState, stateReward, done, info = self.env.step(self.currAction)\n",
    "        self.endOfEp = done\n",
    "        \n",
    "        return [nextState, stateReward]\n",
    "    \n",
    "    def updateQ(self, alpha, delta):\n",
    "        self.qValues += delta * alpha * self.eligibility\n",
    "        \n",
    "    def findBestAction(self, ep, state):\n",
    "        stateQValues = self.qValues[state]\n",
    "        numActions = self.env.action_space.n\n",
    "        pValues = epGreedy(stateQValues, numActions, ep)\n",
    "        \n",
    "        return chooseBestAction(pValues)\n",
    "        \n",
    "    def updateElig(self):\n",
    "        self.eligibility = self.gamma * self.algoLambda * self.eligibility\n",
    "        \n",
    "    def goToNextState(self, nextState, nextAction):\n",
    "        self.currState = nextState\n",
    "        self.currAcion = nextAction\n",
    "        self.currEpSteps += 1\n",
    "        \n",
    "    def calcEpRegret(self, epIndex):\n",
    "        if self.maxReward == 0:\n",
    "            regret = 98 * epIndex - self.totalRewards\n",
    "        else:\n",
    "            regret = self.maxReward * epIndex - self.totalRewards\n",
    "        self.allRegrets.append(regret)\n",
    "        \n",
    "    def plotRegret(self):\n",
    "        print(' \\t Leaning curve for lambda=',self.algoLambda)\n",
    "        plot1, = plt.plot([i for i in range(len(self.allRegrets))], self.allRegrets)\n",
    "        plt.setp(plot1, 'color', 'orchid')\n",
    "        plt.show()\n",
    "\n",
    "    def doAlgorithm(self):\n",
    "        \n",
    "        for i in range(self.numEpisode):\n",
    "            #Init state\n",
    "            self.currState = self.env.reset()\n",
    "            self.currEpSteps = 0\n",
    "            self.eligibility = np.zeros((env.observation_space.n,self.env.action_space.n))\n",
    "            epReward = 0\n",
    "            ep = 1/(i+1)\n",
    "            \n",
    "            #Choosing action in state\n",
    "            self.currAction = self.findBestAction(ep, self.currState)\n",
    "\n",
    "\n",
    "            while self.currEpSteps < 100:\n",
    "                \n",
    "                #Take action and observe s' and r\n",
    "                nextState, stateReward = self.takeCurrAction()\n",
    "                self.totalRewards += stateReward\n",
    "                epReward += stateReward\n",
    "                \n",
    "                #Choosing next action in next state\n",
    "                nextAction = self.findBestAction(ep, nextState)\n",
    "                \n",
    "                #Find delta\n",
    "                delta = stateReward + self.gamma * self.qValues[nextState][nextAction] - self.qValues[self.currState][self.currAction]\n",
    "                \n",
    "                #Update eligibility of selected action and state\n",
    "                self.eligibility[self.currState][self.currAction] = 1\n",
    "\n",
    "                #Update Q for all state-actions so far\n",
    "                self.updateQ(0.1, delta)\n",
    "                \n",
    "                #Update eilibility trace for all state-actions so far\n",
    "                self.updateElig()\n",
    "                \n",
    "                #Go to next state\n",
    "                self.goToNextState(nextState, nextAction)\n",
    "                \n",
    "            if self.maxReward < epReward:\n",
    "                self.maxReward = epReward\n",
    "                \n",
    "            self.calcEpRegret(i+1)    \n",
    "            self.endOfEp = False\n",
    "\n",
    "\n",
    "        \n",
    "numEp = 1000\n",
    "start_time = timeit.default_timer()\n",
    "lambda1 = 1\n",
    "mySARSALambda = SARSALambda(env, lambda1, numEp)\n",
    "mySARSALambda.doAlgorithm()\n",
    "mySARSALambda.plotRegret()\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print(' \\n 1000 pisodes completed in {:.2f}s for lambda=1'.format(elapsed_time))\n",
    "\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "lambda2 = 0.3\n",
    "mySARSALambda = SARSALambda(env, lambda2, numEp)\n",
    "mySARSALambda.doAlgorithm()\n",
    "mySARSALambda.plotRegret()\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print(' \\n 1000 pisodes completed in {:.2f}s for lambda=0.3'.format(elapsed_time))\n",
    "\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "lambda3 = 0\n",
    "mySARSALambda = SARSALambda(env, lambda3, numEp)\n",
    "mySARSALambda.doAlgorithm()\n",
    "mySARSALambda.plotRegret()\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print(' \\n 1000 pisodes completed in {:.2f}s for lambda=0'.format(elapsed_time))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
